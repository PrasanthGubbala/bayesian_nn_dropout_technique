---
title: "Regularization of Bayesian NN by Dropout Technique"
subtitle: "INFO 510 - Fall 2024 - Final Project"
author: 
  - name: "Dropout Dynamics"
    affiliations:
      - name: "School of Information, University of Arizona"
description: "This project aims to predict whether a loan applicant will be approved for a personal loan using a Bayesian Neural Network (BNN) with Monte Carlo Dropout regularization. The BNN will be trained on a dataset of 5,000 samples, incorporating 12 financial variables, such as age, income, credit score, and account types, to predict the binary outcome of loan approval. By incorporating dropout during both training and inference, the model aims to enhance generalization and provide uncertainty estimates for its predictions. The project explores how effectively a neural network can predict loan approval based on financial metadata, and examines the impact of Monte Carlo Dropout in regulating the model, preventing overfitting, and improving overall prediction performance. The ultimate goal is to demonstrate how BNNs, when combined with dropout regularization, can be leveraged to improve financial decision-making processes."
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
---

## Abstract

This project aims to develop a **Bayesian Neural Network** with **Monte Carlo Dropout** to predict whether a loan applicant will be approved for a personal loan, using a dataset of 5,000 samples. The dataset includes 12 variables representing various aspects of the applicant's financial profile, including age, income, credit score, and account types. The model will incorporate dropout regularization to prevent overfitting and improve generalization. The goal is to evaluate the model's ability to accurately predict loan approval and assess how well Monte Carlo Dropout regulates the model, preventing overfitting and enhancing prediction accuracy. This approach offers insights into the potential of BNNs in financial decision-making and the effectiveness of dropout techniques in improving model performance.

## Introduction

The ability to accurately predict loan approval is a critical aspect of financial decision-making. This project aims to develop a **Bayesian Neural Network (BNN)** to predict whether a loan applicant will be approved for a personal loan, based on a variety of financial and demographic factors. We will apply Monte Carlo Dropout regularization to enhance the model’s generalization, prevent overfitting, and provide uncertainty estimates in predictions.

Using a dataset of 5,000 loan applicants, with 12 variables such as age, income, credit score, and account types, the model will assist financial institutions in making informed and data-driven decisions regarding loan approvals. The application of BNNs and Monte Carlo Dropout techniques aims to improve prediction accuracy and offer valuable insights into the financial decision-making process.

## Dataset Description

The dataset used in this project is the Bank Loan Approval dataset from Kaggle (2024). It contains 5,000 stratified samples of loan applicants, with various financial and demographic features. This dataset is ideal for predicting whether an applicant will be approved for a personal loan based on multiple variables, including income, credit score, and other financial indicators.

### Key Variables:

1.  **Age** – Age of the loan applicant (23 - 67 years).
2.  **Years of Work Experience** – Total years of work experience (0 - 43 years).
3.  **Income** – Annual income of the applicant (\$8,000 - \$224,000).
4.  **Number of Family Members** – Number of people in the applicant's family (1 - 4 members).
5.  **Average Credit Card Score** – Credit score on a scale of 0 to 10.
6.  **Education Level** – Education level of the applicant (1 - 3, with 1 being least educated).
7.  **Value of Home Mortgage** – Mortgage value of the applicant’s home (\$0 - \$635,000).
8.  **Personal Loan Approved** – Binary outcome indicating if the personal loan was approved (0 = No, 1 = Yes) (response variable).
9.  **Has a Securities Account** – Whether the applicant has a securities account (0 = No, 1 = Yes).
10. **Has a CD Account** – Whether the applicant has a certificate of deposit (0 = No, 1 = Yes).
11. **Has an Online Banking Account** – Whether the applicant has an online banking account (0 = No, 1 = Yes).
12. **Has a Credit Card** – Whether the applicant has a credit card (0 = No, 1 = Yes).

### Purpose:

These variables provide a comprehensive view of the factors that influence loan approval decisions. By using this dataset, we aim to build a predictive model that helps financial institutions assess the likelihood of loan approval based on an applicant’s financial and demographic information. The goal is to automate and optimize the loan approval process, enabling faster, data-driven decisions that improve operational efficiency and reduce risk for lenders.

## Research Questions

The primary goal of this project is to explore the effectiveness of predicting loan approval decisions using machine learning techniques. Specifically, we aim to address the following research questions:

1.  **How effectively can a neural network predict if a loan applicant will be approved or denied for a personal loan?**\
    This question focuses on evaluating the ability of a **neural network** to learn from various features, such as income, credit score, and years of work experience, to predict loan approval decisions.

2.  **Can the Monte Carlo Dropout technique effectively regulate a neural network and keep it from overfitting the training data?**\
    This question investigates the role of **Monte Carlo Dropout** in enhancing model performance by reducing overfitting, improving generalization, and providing uncertainty estimates for predictions, ensuring that the neural network does not memorize the training data and can generalize better to unseen loan application data.

## Approach Overview

To predict loan approval decisions using a **Bayesian Neural Network (BNN)** with **Monte Carlo Dropout**, we will follow a structured approach involving data preprocessing, model development, and performance evaluation. The goal is to train a neural network to classify whether a loan applicant will be approved or denied, leveraging a stratified dataset with various demographic and financial features.

#### 1. **Data Preprocessing**

-   **Data Cleaning**: Handle missing values, outliers, and ensure consistency across variables to ensure the dataset is accurate and ready for modeling.

-   **Feature Engineering**: Create additional features where necessary, such as aggregating work hours or categorizing mental health-related conditions to provide more relevant insights for the model.

-   **Normalization**: Normalize numerical features to improve model convergence, stability, and ensure that features on different scales do not negatively impact performance.

-   **Encoding**: Convert categorical variables into appropriate formats (e.g., one-hot encoding or label encoding) to ensure the model can process them effectively.

    ```{r}
    # Load the necessary libraries
    library(readr)
    library(dplyr)

    # Load the data
    df <- read_csv("data/bankloan.csv")

    # Review all predictors
    #summary(df)

    # Analyze Mortgage predictor
    # View mortgage $ data
    #range(df$Mortgage) # $0-$635

    # Count the number of values greater than 0 in the Mortgage column
    #count_above_zero <- sum(df$Mortgage > 0, na.rm = TRUE)

    # Get the total number of non-missing values in the column
    #total_values <- sum(!is.na(df$Mortgage))

    # Calculate the proportion
    #proportion_above_zero <- count_above_zero / total_values

    # Print the results
    #count_above_zero # 1,538 ppl with mortgages
    #proportion_above_zero # 30.76% of people have loans

    # Remove Mortgage
    #df <- select(df, -"Mortgage")

    # Normalize using Min-Max Scaling
    df <- df %>%
      mutate(
        Age = (Age - min(Age)) / (max(Age) - min(Age)),
        Experience = (Experience - min(Experience)) / (max(Experience) - min(Experience)),
        Income = (Income - min(Income)) / (max(Income) - min(Income)),
        CCAvg = (CCAvg - min(CCAvg)) / (max(CCAvg) - min(CCAvg)),
        Mortgage = (Mortgage - min(Mortgage, na.rm = TRUE)) / (max(Mortgage, na.rm = TRUE) - min(Mortgage, na.rm = TRUE))
      )

    summary(df)
    ```

    ```{r}
    # Install Keras if not already installed
    # Load the keras library
    library(keras)
    library(dplyr)
    library(tidyr)

    # Install TensorFlow backend (if you haven't already installed it)
    # install_keras()
    ```

    ```{r}
    # Split the data into training and testing sets (80-20 split)
    set.seed(42)  # For reproducibility
    index <- sample(1:nrow(df), size = 0.8 * nrow(df))
    train_data <- df[index, ]
    test_data <- df[-index, ]

    ## Separate features and target labels
    x_train <- as.matrix(train_data[, -8])  # Exclude the 8th column (response variable, Personal Loan)
    y_train <- (train_data$`Personal.Loan`)  # Use Personal Loan as target variable (0 or 1)

    x_test <- as.matrix(test_data[, -8])  # Exclude the 8th column (response variable, Personal Loan)
    y_test <- (test_data$`Personal.Loan`)  # Use Personal Loan as target variable (0 or 1)
    ```

#### 2. **Model Development**

-   **Neural Network Architecture**: Develop a BNN with 4 hidden layers. The network consists of the following layers:

    -   Layer 1: 128 neurons with ReLU activation
    -   Layer 2: 64 neurons with ReLU activation
    -   Layer 3: 32 neurons with ReLU activation
    -   Layer 4: 16 neurons with ReLU activation
    -   Output Layer: 1 neuron with Sigmoid activation for binary classification (loan approval: 0 or 1).

    This architecture is chosen to balance model complexity and training efficiency while ensuring that the model can capture non-linear relationships in the data.

-   **Monte Carlo Dropout**: Implement Monte Carlo Dropout during both training and inference. The dropout technique randomly drops a percentage of neurons during training to prevent overfitting.

-   **Loss Function & Optimization**: Use **binary cross-entropy loss** since the task is a binary classification (loan approval or denial). To minimize the loss function, employ the **Adam optimizer**, which is well-suited for training deep neural networks and adjusts the learning rate during training for efficient convergence.

#### 3. **Model Training**

-   **Data Split**: Split the dataset into an **80/20 train-test split** to evaluate model performance.

-   **Hyperparameter Tuning**: Tune key hyperparameters such as the number of layers, neurons, dropout rate, and learning rate using **k-fold cross-validation**.

-   **Neural Network with MC Dropout**

    ```{r}
    # Define the neural network model with 'x' amount hidden layers and dropout
    model <- keras_model_sequential() %>%
      # First hidden layer with 128 neurons and ReLU activation
      layer_dense(units = 128, activation = "relu", input_shape = dim(x_train)[2]) %>%
      # Dropout layer for regularization (Monte Carlo Dropout)
      layer_dropout(rate = 0.3) %>%
      
      # Second hidden layer with 64 neurons and ReLU activation
      layer_dense(units = 64, activation = "relu") %>%
      # Dropout layer again
      layer_dropout(rate = 0.4) %>%
      
      # third hidden layer with 32 neurons and ReLU activation
      layer_dense(units = 32, activation = "relu") %>%
      # Dropout layer again
      layer_dropout(rate = 0.5) %>%
      
      # fourth hidden layer with 16 neurons and ReLU activation
      layer_dense(units = 16, activation = "relu") %>%
      # Dropout layer again
      layer_dropout(rate = 0.6) %>%
      
      # Output layer with Sigmoid activation for binary classification
      layer_dense(units = 1, activation = "sigmoid")

    ## Compile the model with Adam optimizer and binary crossentropy loss
    model %>% compile(
      loss = "binary_crossentropy",
      optimizer = optimizer_adam(learning_rate = 0.001),
      metrics = c("accuracy")
    )

    # Display the model summary
    summary(model)
    ```

-   **Model Training**

    ```{r}
    history <- model %>% fit(
      x_train, y_train,
      epochs = 30,
      batch_size = 5,
      validation_data = list(x_test, y_test)
    )
    ```

    ```{r}
    # Ensure history metrics are in the right format
    history_df <- data.frame(epoch = 1:length(history$metrics$loss),
                             loss = history$metrics$loss,
                             val_loss = history$metrics$val_loss,
                             accuracy = history$metrics$accuracy,
                             val_accuracy = history$metrics$val_accuracy)
    ```

    ```{r}
    library(ggplot2)

    # Plot Loss and Validation Loss
    loss_plot <- ggplot(history_df, aes(x = epoch)) +
      geom_line(aes(y = loss, color = "Training Loss")) +
      geom_line(aes(y = val_loss, color = "Validation Loss")) +
      labs(title = "Training and Validation Loss", x = "Epoch", y = "Loss") +
      scale_color_manual(values = c("Training Loss" = "blue", "Validation Loss" = "red")) +
      theme_minimal()

    print(loss_plot)

    # ggsave("loss_plot.png", plot = loss_plot, width = 8, height = 6)̧
    ```

    ```{r}
    # Plot Accuracy and Validation Accuracy
    accuracy_plot <- ggplot(history_df, aes(x = epoch)) +
      geom_line(aes(y = accuracy, color = "Training Accuracy")) +
      geom_line(aes(y = val_accuracy, color = "Validation Accuracy")) +
      labs(title = "Training and Validation Accuracy", x = "Epoch", y = "Accuracy") +
      scale_color_manual(values = c("Training Accuracy" = "green", "Validation Accuracy" = "orange")) +
      theme_minimal()

    # Display the plot
    print(accuracy_plot)

    # Save the plot to a file
    # ggsave("accuracy_plot.png", plot = accuracy_plot, width = 8, height = 6, dpi = 300)̧
    ```

#### 4. **Performance Evaluation**

-   **Accuracy**: Measure model performance using accuracy, precision, recall, and F1-score for each stress level category (low, medium, high).

-   **Uncertainty Estimation**: Assess the uncertainty in predictions using Monte Carlo Dropout by averaging over multiple forward passes.

-   **Model Interpretability**: Explore which features contribute most to the model’s predictions using techniques like **SHAP values**.

    ```{r}
    model %>% evaluate(x_test, y_test)
    ```

### 5. **Insights and Application**

-   A BNN model has a high degree of accuracy in predicting classifations labels and is a good candidate to use alongside other classification models, such as Naive Bayes, Gradient Boosting, Support Vector Machines, etc.

-   In the case of this dataset, financial institutions and loan originators can use a BNN model as supplemental analysis in their loan approval process, or use it to forecast changes in a loan porfolio. Likewise, consumers can use a BNN to analyze their chances of being approved for a loan and adjust variables on their end.

    ```{r}
    predictions <- model %>% predict(x_test)
    predicted_classes <- ifelse(predictions > 0.5, 1, 0)

    # Reshape predicted_classes into a matrix with 10 columns
    reshaped_predictions <- matrix(predicted_classes, ncol = 10, byrow = TRUE)

    # Print the reshaped predictions
    print(reshaped_predictions)

    # Flatten the data frame into a single vector
    prop_of_loan_approvals <- as.vector(reshaped_predictions)

    # Count the number of 1's and 0's
    counts <- table(prop_of_loan_approvals)
    print(counts)
    ```

## Model Architecture:

-   **Input Layer**: Takes features from `x_train`.
-   **Hidden Layers**: Four layers with 128, 64, 32, and 16 neurons, using **ReLU** activation.
-   **Dropout**: Dropout applied after each layer with rates 0.3, 0.4, 0.5, and 0.6 to prevent overfitting. **Monte Carlo Dropout** keeps dropout active during inference for uncertainty estimation.

### Output Layer:

-   **Softmax** activation for predicting three classes: low, medium, and high stress.

### Training:

-   **Loss**: Categorical cross-entropy for multi-class classification.
-   **Optimizer**: Adam.
-   **Epochs**: 50 with batch size 5.

## Conclusion:

We successfully produced a BNN that accurately predicts classification labels for a multi-variable dataset and keep said model from overfitting the data by utilizing the MC Dropout technique.

While our model was used specifically to determine if a loan applicant would be approved for a personal loan, it can also be used to determine if an applicant will be approved for any type of loan (car, mortgage, business etc.) or applied to any field that has a sufficiently-sampled, correlated dataset, and still provide a high degree of accuracy in predicting classification labels.
