---
title: "Regularization of Bayesian NN by Dropout Technique"
subtitle: "INFO 510 - Fall 2024 - Final Project"
author: 
  - name: "Dropout Dynamics"
    affiliations:
      - name: "School of Information, University of Arizona"
description: "This project aims to predict employee stress levels (low, medium, or high) using a neural network enhanced with Monte Carlo Dropout regularization. By applying dropout during both training and inference, we aim to improve the model's generalization and estimate prediction uncertainty. The model will be trained on a diverse dataset of 5,000 employee samples, incorporating factors like age, job role, hours worked, and mental health conditions. The project explores how effectively a neural network can predict stress based on employee metadata and the impact of dropout regularization on model performance, ultimately helping organizations better support employee well-being."
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
---

## Abstract

This project aims to develop a **Bayesian Neural Network** with **Monte Carlo Dropout** to predict employee stress levels (low, medium, high) based on metadata such as age, gender, job role, industry, and work hours. The model will utilize a stratified dataset of 5,000 employee samples, with 19 variables representing different aspects of employee well-being. By incorporating dropout regularization, the model will enhance generalization and estimate prediction uncertainty. The goal is to evaluate how well the network can predict stress levels and the impact of dropout on model performance, ultimately providing organizations with insights to better support employee well-being.

## Introduction

Employee stress significantly impacts productivity and well-being in the workplace. This project aims to develop a **Bayesian Neural Network** (BNN) to predict employee stress levels (low, medium, high) using demographic and job-related data. We will apply **Monte Carlo Dropout** to regularize the model, improving performance and providing uncertainty estimates in predictions.

Using a stratified dataset of 5,000 employees, the model will help organizations predict stress levels and make data-driven decisions to support employee well-being.

## Dataset Description

The dataset used in this project is the **Remote Work & Mental Health** dataset from Kaggle (2024). It includes **5,000 stratified samples** of employees across various industries and countries. The dataset captures a wide range of factors that influence employee stress, making it ideal for predicting stress levels based on multiple variables.

### Key Variables:

1.  **Age** – Age of the employee.
2.  **Gender** – Gender of the employee (e.g., Male, Female, Non-binary).
3.  **Job Role** – Role of the employee (e.g., HR, Data Scientist, Software Engineer).
4.  **Industry** – Industry the employee works in (e.g., Healthcare, IT, Manufacturing).
5.  **Years of Experience** – Total years the employee has worked.
6.  **Work Location** – Type of work arrangement (Remote, Hybrid, Onsite).
7.  **Hours Worked Per Week** – Average number of hours worked weekly.
8.  **Number of Virtual Meetings** – Frequency of virtual meetings the employee attends.
9.  **Work Life Balance Rating** – Employee's self-assessed work-life balance.
10. **Stress Level** – Stress level (Low, Medium, High) (response variable).
11. **Mental Health Condition** – Whether the employee has a reported mental health condition.
12. **Access to Mental Health Resources** – Whether the employee has access to mental health resources.
13. **Productivity Change** – Change in productivity (Increase, Decrease, No Change).
14. **Social Isolation Rating** – Employee's rating of social isolation.
15. **Satisfaction with Remote Work** – How satisfied the employee is with remote work.
16. **Company Support for Remote Work** – Support from the company for remote work.
17. **Physical Activity** – Frequency of physical activity (e.g., Daily, Weekly).
18. **Sleep Quality** – Employee's self-assessed sleep quality.
19. **Region** – Geographic region of the employee (e.g., Europe, North America, Asia).

### Purpose:

These variables provide a comprehensive view of the factors that contribute to employee stress. By using this dataset, we can build a predictive model that helps organizations understand the key factors affecting employee well-being and tailor interventions accordingly.

## Research Questions

The primary goal of this project is to explore the effectiveness of predicting employee stress levels using machine learning techniques. Specifically, we aim to address the following research questions:

1.  **How effectively can a neural network predict employee stress levels worldwide based on individual and career-related metadata?**\
    This question focuses on evaluating the ability of a neural network to learn from diverse features, such as age, job role, and mental health, to predict stress levels across a global workforce.

2.  **How does the Monte Carlo Dropout technique impact the performance and optimization of a neural network in predicting stress levels?**\
    This question explores the effect of **Monte Carlo Dropout** on model performance by improving generalization, reducing overfitting, and providing uncertainty estimates for predictions.

## Approach Overview

To predict employee stress levels using a **Bayesian Neural Network** (BNN) with **Monte Carlo Dropout**, we will follow a structured approach involving data preprocessing, model development, and performance evaluation.

#### 1. **Data Preprocessing**

-   **Data Cleaning**: Handle missing values, outliers, and ensure consistency across variables.
-   **Feature Engineering**: Create additional features if necessary, such as aggregating work hours or categorizing health-related conditions.
-   **Normalization**: Normalize numerical features (e.g., age, hours worked) to improve model convergence and stability.
-   **Encoding**: Convert categorical variables (e.g., gender, job role, work location) into appropriate formats (e.g., one-hot encoding or label encoding).

#### 2. **Model Development**

-   **Neural Network Architecture**: Build a feedforward neural network with 2 hidden layers and 50 neurons per layer. Use the **ReLU activation** function for the hidden layers and a **Softmax activation** function for the output layer.
-   **Monte Carlo Dropout**: Implement Monte Carlo Dropout during both training and inference to reduce overfitting and provide uncertainty estimates for predictions.
-   **Loss Function & Optimization**: Use **cross-entropy loss** for classification and the **Adam optimizer** to minimize the loss.

#### 3. **Model Training**

-   **Data Split**: Split the dataset into an **80/20 train-test split** to evaluate model performance.
-   **Hyperparameter Tuning**: Tune key hyperparameters such as the number of layers, neurons, dropout rate, and learning rate using **k-fold cross-validation**.

#### 4. **Performance Evaluation**

-   **Accuracy**: Measure model performance using accuracy, precision, recall, and F1-score for each stress level category (low, medium, high).
-   **Uncertainty Estimation**: Assess the uncertainty in predictions using Monte Carlo Dropout by averaging over multiple forward passes.
-   **Model Interpretability**: Explore which features contribute most to the model’s predictions using techniques like **SHAP values**.

### 5. **Insights and Application**

-   Provide actionable insights for organizations to improve employee well-being based on predicted stress levels and key contributing factors.
-   Evaluate the impact of various metadata (e.g., job role, work location, mental health conditions) on stress predictions.

## 1. Data Loading & Preprocessing

```{r}
# Load the necessary libraries
library(readr)
library(dplyr)

# Load the data
df <- read_csv("data/bankloan.csv")

# Review all predictors
#summary(df)

# Analyze Mortgage predictor
# View mortgage $ data
#range(df$Mortgage) # $0-$635

# Count the number of values greater than 0 in the Mortgage column
#count_above_zero <- sum(df$Mortgage > 0, na.rm = TRUE)

# Get the total number of non-missing values in the column
#total_values <- sum(!is.na(df$Mortgage))

# Calculate the proportion
#proportion_above_zero <- count_above_zero / total_values

# Print the results
#count_above_zero # 1,538 ppl with mortgages
#proportion_above_zero # 30.76% of people have loans

# Remove Mortgage
#df <- select(df, -"Mortgage")

# Normalize using Min-Max Scaling
df <- df %>%
  mutate(
    Age = (Age - min(Age)) / (max(Age) - min(Age)),
    Experience = (Experience - min(Experience)) / (max(Experience) - min(Experience)),
    Income = (Income - min(Income)) / (max(Income) - min(Income)),
    CCAvg = (CCAvg - min(CCAvg)) / (max(CCAvg) - min(CCAvg)),
    Mortgage = (Mortgage - min(Mortgage, na.rm = TRUE)) / (max(Mortgage, na.rm = TRUE) - min(Mortgage, na.rm = TRUE))
  )

summary(df)
```

```{r}
# Install Keras if not already installed
# Load the keras library
library(keras)
library(dplyr)
library(tidyr)

# Install TensorFlow backend (if you haven't already installed it)
# install_keras()
```

```{r}
# Split the data into training and testing sets (80-20 split)
set.seed(42)  # For reproducibility
index <- sample(1:nrow(df), size = 0.8 * nrow(df))
train_data <- df[index, ]
test_data <- df[-index, ]

## Separate features and target labels
x_train <- as.matrix(train_data[, -8])  # Exclude the 8th column (response variable, Personal Loan)
y_train <- (train_data$`Personal.Loan`)  # Use Personal Loan as target variable (0 or 1)

x_test <- as.matrix(test_data[, -8])  # Exclude the 8th column (response variable, Personal Loan)
y_test <- (test_data$`Personal.Loan`)  # Use Personal Loan as target variable (0 or 1)
```

## 2. Neural Network with MC Dropout

```{r}
# Define the neural network model with 'x' amount hidden layers and dropout
model <- keras_model_sequential() %>%
  # First hidden layer with 128 neurons and ReLU activation
  layer_dense(units = 128, activation = "relu", input_shape = dim(x_train)[2]) %>%
  # Dropout layer for regularization (Monte Carlo Dropout)
  layer_dropout(rate = 0.3) %>%
  
  # Second hidden layer with 64 neurons and ReLU activation
  layer_dense(units = 64, activation = "relu") %>%
  # Dropout layer again
  layer_dropout(rate = 0.4) %>%
  
  # third hidden layer with 32 neurons and ReLU activation
  layer_dense(units = 32, activation = "relu") %>%
  # Dropout layer again
  layer_dropout(rate = 0.5) %>%
  
  # third hidden layer with 16 neurons and ReLU activation
  layer_dense(units = 16, activation = "relu") %>%
  # Dropout layer again
  layer_dropout(rate = 0.6) %>%
  
  # Output layer with Sigmoid activation for binary classification
  layer_dense(units = 1, activation = "sigmoid")

## Compile the model with Adam optimizer and binary crossentropy loss
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c("accuracy")
)

# Display the model summary
summary(model)
```

## 3. **Model Training**

```{r}
history <- model %>% fit(
  x_train, y_train,
  epochs = 30,
  batch_size = 5,
  validation_data = list(x_test, y_test)
)
```

```{r}
# Ensure history metrics are in the right format
history_df <- data.frame(epoch = 1:length(history$metrics$loss),
                         loss = history$metrics$loss,
                         val_loss = history$metrics$val_loss,
                         accuracy = history$metrics$accuracy,
                         val_accuracy = history$metrics$val_accuracy)


```

```{r}
library(ggplot2)

# Plot Loss and Validation Loss
ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = loss, color = "Training Loss")) +
  geom_line(aes(y = val_loss, color = "Validation Loss")) +
  labs(title = "Loss and Validation Loss", x = "Epoch", y = "Loss") +
  scale_color_manual(values = c("Training Loss" = "blue", "Validation Loss" = "red")) +
  theme_minimal()


```

```{r}
# Plot Accuracy and Validation Accuracy
ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = accuracy, color = "Training Accuracy")) +
  geom_line(aes(y = val_accuracy, color = "Validation Accuracy")) +
  labs(title = "Accuracy and Validation Accuracy", x = "Epoch", y = "Accuracy") +
  scale_color_manual(values = c("Training Accuracy" = "green", "Validation Accuracy" = "orange")) +
  theme_minimal()
```

## 4. **Performance Evaluation**

```{r}
model %>% evaluate(x_test, y_test)
```

## 5. **Insights and Application**

```{r}
predictions <- model %>% predict(x_test)
predicted_classes <- max.col(predictions)
print(predicted_classes)
```

## Model Architecture:

-   **Input Layer**: Takes features from `x_train`.
-   **Hidden Layers**: Four layers with 128, 64, 32, and 16 neurons, using **ReLU** activation.
-   **Dropout**: Dropout applied after each layer with rates 0.3, 0.4, 0.5, and 0.6 to prevent overfitting. **Monte Carlo Dropout** keeps dropout active during inference for uncertainty estimation.

### Output Layer:

-   **Softmax** activation for predicting three classes: low, medium, and high stress.

### Training:

-   **Loss**: Categorical cross-entropy for multi-class classification.
-   **Optimizer**: Adam.
-   **Epochs**: 50 with batch size 5.

## Conclusion:

The model uses **Monte Carlo Dropout** for regularization and uncertainty estimation, which improves generalization and provides confidence in predictions, crucial for applications like predicting employee stress levels.

DCA, see what variables are imprortant. Bayesian linear regression.
