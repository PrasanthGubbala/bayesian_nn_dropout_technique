---
title: "Regularization of Bayesian Neural Networks with Dropout Nodes"
subtitle: "INFO 510 - Fall 2023 - Final Project"
author: "Author names"
title-slide-attributes:
  data-background-image: images/watercolour_sys02_img34_teacup-ocean.jpg
  data-background-size: stretch
  data-background-opacity: "0.7"
  data-slide-number: none
format:
  revealjs:
    theme:  ['data/customtheming.scss']
  
editor: visual
execute:
  echo: false
---

```{r}
#| label: load-packages
#| include: false

# Load packages here
pacman::p_load(tidymodels,
               tidyverse)

```

```{r}
#| label: setup
#| include: false

# Plot theme
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 11))

# For better figure resolution
knitr::opts_chunk$set(
  fig.retina = 3, 
  dpi = 300, 
  fig.width = 6, 
  fig.asp = 0.618 
  )
```

```{r}
#| label: load-data
#| include: false
# Load data here
data(mtcars)
mtcars$speed <- mtcars$hp / mtcars$wt

data("penguins")
```

```{r}
# Load the necessary libraries
library(readr)
library(dplyr)

# Load the data
employee_data <- read_csv("data/Impact_of_Remote_Work_on_Mental_Health.csv")

# Check the first few rows of the data
# head(employee_data)

# Check for missing values
# summary(employee_data)

# colSums(is.na(employee_data)) 

# Example of replacing NA in 'Mental_Health_Condition' with 'Unknown'
employee_data$Mental_Health_Condition[is.na(employee_data$Mental_Health_Condition)] <- "Unknown"

# If you want to remove rows with NA values
employee_data <- na.omit(employee_data)



# Load required package
library(dplyr)

# Function to remove outliers using IQR for all numeric columns
remove_outliers_all <- function(df) {
  # Loop over each column in the data frame
  for (col_name in names(df)) {
    # Check if the column is numeric
    if (is.numeric(df[[col_name]])) {
      Q1 <- quantile(df[[col_name]], 0.25, na.rm = TRUE)
      Q3 <- quantile(df[[col_name]], 0.75, na.rm = TRUE)
      IQR_value <- Q3 - Q1
      lower_bound <- Q1 - 1.5 * IQR_value
      upper_bound <- Q3 + 1.5 * IQR_value
      
      # Filter rows where values are within the bounds
      df <- df %>% filter(df[[col_name]] >= lower_bound & df[[col_name]] <= upper_bound)
    }
  }
  return(df)
}

# Example: Removing outliers in all numeric columns of employee_data
employee_data_clean <- remove_outliers_all(employee_data)

# Print the cleaned data
# print(employee_data_clean)


# Step 1: Select the relevant features
df_relevant <- employee_data_clean %>%
  select(Work_Location, Hours_Worked_Per_Week, Work_Life_Balance_Rating, 
         Mental_Health_Condition, Social_Isolation_Rating, 
         Satisfaction_with_Remote_Work, Sleep_Quality, Stress_Level)  # assuming 'Stress_Level' is the target variable


# Step 4: Feature Engineering
# Create a new 'Health_Condition_Severity' feature based on 'Mental_Health_Condition'
df_relevant$Health_Condition_Severity <- ifelse(df_relevant$Mental_Health_Condition %in% c("Depression", "Burnout", "Anxiety"), "High", "Low")

# Categorize 'Hours_Worked_Per_Week' into ranges (Low, Medium, High)
df_relevant$Work_Hours_Category <- cut(df_relevant$Hours_Worked_Per_Week, 
                                       breaks = c(0, 35, 45, Inf), 
                                       labels = c("Low", "Medium", "High"))

df = df_relevant


# Convert the target variable to a factor & factors into dummy variables (one-hot encoding)
# 0, 1, 2 for Setosa, Versicolor, Virginica
df$Work_Location <- as.factor(df$Work_Location)
df$Work_Location <- as.numeric(df$Work_Location) - 1  

df$Mental_Health_Condition <- as.factor(df$Mental_Health_Condition)
df$Mental_Health_Condition <- as.numeric(df$Mental_Health_Condition) - 1 

df$Satisfaction_with_Remote_Work <- as.factor(df$Satisfaction_with_Remote_Work)
df$Satisfaction_with_Remote_Work <- as.numeric(df$Satisfaction_with_Remote_Work) - 1 

df$Sleep_Quality <- as.factor(df$Sleep_Quality)
df$Sleep_Quality <- as.numeric(df$Sleep_Quality) - 1 

df$Stress_Level <- as.factor(df$Stress_Level)
df$Stress_Level <- as.numeric(df$Stress_Level) - 1 

df$Health_Condition_Severity <- as.factor(df$Health_Condition_Severity)
df$Health_Condition_Severity <- as.numeric(df$Health_Condition_Severity) - 1 

df$Work_Hours_Category <- as.factor(df$Work_Hours_Category)
df$Work_Hours_Category <- as.numeric(df$Work_Hours_Category) - 1 



# Normalize the features
df_without_target <- as.data.frame(scale(df[, setdiff(names(df), "Stress_Level")]))  # Only scale the features

# Combine the scaled features and the target label
df <- cbind(df_without_target, Stress_Level = df$Stress_Level)


# Install Keras if not already installed
# Load the keras library
library(keras)
library(dplyr)
library(tidyr)

# Install TensorFlow backend (if you haven't already installed it)
# install_keras()


# Split the data into training and testing sets (80-20 split)
set.seed(42)  # For reproducibility
index <- sample(1:nrow(df), size = 0.8 * nrow(df))
train_data <- df[index, ]
test_data <- df[-index, ]

# Separate features and target labels
x_train <- as.matrix(train_data[, 1:9])
y_train <- to_categorical(train_data$Stress_Level, 3)  # Convert to one-hot encoding

x_test <- as.matrix(test_data[, 1:9])
y_test <- to_categorical(test_data$Stress_Level, 3)

```

## Neural Network with MC Dropout  

```{r}
# Define the neural network model with 2 hidden layers and dropout
model <- keras_model_sequential() %>%
  # First hidden layer with 128 neurons and ReLU activation
  layer_dense(units = 128, activation = "relu", input_shape = dim(x_train)[2]) %>%
  # Dropout layer for regularization (Monte Carlo Dropout)
  layer_dropout(rate = 0.3) %>%
  
  # Second hidden layer with 64 neurons and ReLU activation
  layer_dense(units = 64, activation = "relu") %>%
  # Dropout layer again
  layer_dropout(rate = 0.4) %>%
  
  # third hidden layer with 32 neurons and ReLU activation
  layer_dense(units = 32, activation = "relu") %>%
  # Dropout layer again
  layer_dropout(rate = 0.5) %>%
  
  # third hidden layer with 16 neurons and ReLU activation
  layer_dense(units = 16, activation = "relu") %>%
  # Dropout layer again
  layer_dropout(rate = 0.6) %>%
  
  # Output layer with Softmax activation for multi-class classification
  layer_dense(units = 3, activation = "softmax")

# Compile the model with cross-entropy loss and Adam optimizer
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(),
  metrics = c("accuracy")
)

# Display the model summary
summary(model)
```

## **Model Training**

```{r}
history <- model %>% fit(
  x_train, y_train,
  epochs = 50,
  batch_size = 5,
  validation_data = list(x_test, y_test)
)
```

```{r}
# Ensure history metrics are in the right format
history_df <- data.frame(epoch = 1:length(history$metrics$loss),
                         loss = history$metrics$loss,
                         val_loss = history$metrics$val_loss,
                         accuracy = history$metrics$accuracy,
                         val_accuracy = history$metrics$val_accuracy)

```

## Loss and Validation Loss

```{r}
library(ggplot2)

# Plot Loss and Validation Loss
ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = loss, color = "Training Loss")) +
  geom_line(aes(y = val_loss, color = "Validation Loss")) +
  labs(title = "Loss and Validation Loss", x = "Epoch", y = "Loss") +
  scale_color_manual(values = c("Training Loss" = "blue", "Validation Loss" = "red")) +
  theme_minimal()
```

## Accuracy and Validation Accuracy

```{r}
# Plot Accuracy and Validation Accuracy
ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = accuracy, color = "Training Accuracy")) +
  geom_line(aes(y = val_accuracy, color = "Validation Accuracy")) +
  labs(title = "Accuracy and Validation Accuracy", x = "Epoch", y = "Accuracy") +
  scale_color_manual(values = c("Training Accuracy" = "green", "Validation Accuracy" = "orange")) +
  theme_minimal()
```

## **Performance Evaluation**

```{r}
model %>% evaluate(x_test, y_test)
```

## **Insights and Application**

```{r}
predictions <- model %>% predict(x_test)
predicted_classes <- max.col(predictions) - 1  # Convert from one-hot to class index
print(predicted_classes)

```
