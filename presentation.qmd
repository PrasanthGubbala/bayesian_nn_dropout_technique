---
title: "Regularization of Bayesian Neural Networks with Dropout Nodes"
subtitle: "INFO 510 Bayesian Modeling & Inference - Fall 2023 - Final Project"
author: ["Gubbala Durga Prasanth", "Kendall Beaver", "V.S. Murali Krishna Chittlu"]
title-slide-attributes:
  data-background-image: images/watercolour_sys02_img34_teacup-ocean.jpg
  data-background-size: stretch
  data-background-opacity: "0.7"
  data-slide-number: none
format:
  revealjs:
    theme:  ['data/customtheming.scss']
  
editor: visual
execute:
  echo: false
---

```{r}
#| label: load-packages
#| include: false

# Load packages here
pacman::p_load(tidymodels,
               tidyverse)

# Install Keras if not already installed
# Load the keras library
library(keras)
library(dplyr)
library(tidyr)

# Install TensorFlow backend (if you haven't already installed it)
# install_keras()

```

```{r}
#| label: setup
#| include: false

# Plot theme
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 11))

# For better figure resolution
knitr::opts_chunk$set(
  fig.retina = 3, 
  dpi = 300, 
  fig.width = 6, 
  fig.asp = 0.618 
  )
```

```{r}
# Load the necessary libraries
library(readr)
library(dplyr)

# Load the data
df <- read_csv("data/bankloan.csv")

# Review all predictors
#summary(df)

# Analyze Mortgage predictor
# View mortgage $ data
#range(df$Mortgage) # $0-$635

# Count the number of values greater than 0 in the Mortgage column
#count_above_zero <- sum(df$Mortgage > 0, na.rm = TRUE)

# Get the total number of non-missing values in the column
#total_values <- sum(!is.na(df$Mortgage))

# Calculate the proportion
#proportion_above_zero <- count_above_zero / total_values

# Print the results
#count_above_zero # 1,538 ppl with mortgages
#proportion_above_zero # 30.76% of people have loans

# Remove Mortgage
#df <- select(df, -"Mortgage")

# Normalize using Min-Max Scaling
df <- df %>%
  mutate(
    Age = (Age - min(Age)) / (max(Age) - min(Age)),
    Experience = (Experience - min(Experience)) / (max(Experience) - min(Experience)),
    Income = (Income - min(Income)) / (max(Income) - min(Income)),
    CCAvg = (CCAvg - min(CCAvg)) / (max(CCAvg) - min(CCAvg)),
    Mortgage = (Mortgage - min(Mortgage, na.rm = TRUE)) / (max(Mortgage, na.rm = TRUE) - min(Mortgage, na.rm = TRUE))
  )

summary(df)
```

```{r}
# Split the data into training and testing sets (80-20 split)
set.seed(42)  # For reproducibility
index <- sample(1:nrow(df), size = 0.8 * nrow(df))
train_data <- df[index, ]
test_data <- df[-index, ]

## Separate features and target labels
x_train <- as.matrix(train_data[, -8])  # Exclude the 8th column (response variable, Personal Loan)
y_train <- (train_data$`Personal.Loan`)  # Use Personal Loan as target variable (0 or 1)

x_test <- as.matrix(test_data[, -8])  # Exclude the 8th column (response variable, Personal Loan)
y_test <- (test_data$`Personal.Loan`)  # Use Personal Loan as target variable (0 or 1)
```

## Neural Network with MC Dropout

```{r}
# Define the neural network model with 'x' amount hidden layers and dropout
model <- keras_model_sequential() %>%
  # First hidden layer with 128 neurons and ReLU activation
  layer_dense(units = 128, activation = "relu", input_shape = dim(x_train)[2]) %>%
  # Dropout layer for regularization (Monte Carlo Dropout)
  layer_dropout(rate = 0.3) %>%
  
  # Second hidden layer with 64 neurons and ReLU activation
  layer_dense(units = 64, activation = "relu") %>%
  # Dropout layer again
  layer_dropout(rate = 0.4) %>%
  
  # third hidden layer with 32 neurons and ReLU activation
  layer_dense(units = 32, activation = "relu") %>%
  # Dropout layer again
  layer_dropout(rate = 0.5) %>%
  
  # fourth hidden layer with 16 neurons and ReLU activation
  layer_dense(units = 16, activation = "relu") %>%
  # Dropout layer again
  layer_dropout(rate = 0.6) %>%
  
  # Output layer with Sigmoid activation for binary classification
  layer_dense(units = 1, activation = "sigmoid")

## Compile the model with Adam optimizer and binary crossentropy loss
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = c("accuracy")
)

# Display the model summary
summary(model)
```

## **Model Training**

```{r}
history <- model %>% fit(
  x_train, y_train,
  epochs = 30,
  batch_size = 5,
  validation_data = list(x_test, y_test)
)
```

```{r}
# Ensure history metrics are in the right format
history_df <- data.frame(epoch = 1:length(history$metrics$loss),
                         loss = history$metrics$loss,
                         val_loss = history$metrics$val_loss,
                         accuracy = history$metrics$accuracy,
                         val_accuracy = history$metrics$val_accuracy)

```

## Loss and Validation Loss

```{r}
library(ggplot2)

# Plot Loss and Validation Loss
loss_plot <- ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = loss, color = "Training Loss")) +
  geom_line(aes(y = val_loss, color = "Validation Loss")) +
  labs(title = "Training and Validation Loss", x = "Epoch", y = "Loss") +
  scale_color_manual(values = c("Training Loss" = "blue", "Validation Loss" = "red")) +
  theme_minimal()

print(loss_plot)

# ggsave("loss_plot.png", plot = loss_plot, width = 8, height = 6)̧
```

## Accuracy and Validation Accuracy

```{r}
# Plot Accuracy and Validation Accuracy
accuracy_plot <- ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = accuracy, color = "Training Accuracy")) +
  geom_line(aes(y = val_accuracy, color = "Validation Accuracy")) +
  labs(title = "Training and Validation Accuracy", x = "Epoch", y = "Accuracy") +
  scale_color_manual(values = c("Training Accuracy" = "green", "Validation Accuracy" = "orange")) +
  theme_minimal()

# Display the plot
print(accuracy_plot)

# Save the plot to a file
# ggsave("accuracy_plot.png", plot = accuracy_plot, width = 8, height = 6, dpi = 300)̧
```

## **Performance Evaluation**

```{r}
model %>% evaluate(x_test, y_test)
```

## **Insights and Application**

```{r}
predictions <- model %>% predict(x_test)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# Reshape predicted_classes into a matrix with 10 columns
reshaped_predictions <- matrix(predicted_classes, ncol = 10, byrow = TRUE)

# Print the reshaped predictions
print(reshaped_predictions)

# Flatten the data frame into a single vector
prop_of_loan_approvals <- as.vector(reshaped_predictions)

# Count the number of 1's and 0's
counts <- table(prop_of_loan_approvals)
print(counts)

```
