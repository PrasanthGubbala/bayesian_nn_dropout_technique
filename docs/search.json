[
  {
    "objectID": "presentation.html#neural-network-with-mc-dropout",
    "href": "presentation.html#neural-network-with-mc-dropout",
    "title": "Regularization of Bayesian Neural Networks with Dropout Nodes",
    "section": "Neural Network with MC Dropout",
    "text": "Neural Network with MC Dropout\n\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_4 (Dense)                    (None, 128)                     1536        \n dropout_3 (Dropout)                (None, 128)                     0           \n dense_3 (Dense)                    (None, 64)                      8256        \n dropout_2 (Dropout)                (None, 64)                      0           \n dense_2 (Dense)                    (None, 32)                      2080        \n dropout_1 (Dropout)                (None, 32)                      0           \n dense_1 (Dense)                    (None, 16)                      528         \n dropout (Dropout)                  (None, 16)                      0           \n dense (Dense)                      (None, 1)                       17          \n================================================================================\nTotal params: 12417 (48.50 KB)\nTrainable params: 12417 (48.50 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________"
  },
  {
    "objectID": "presentation.html#model-training",
    "href": "presentation.html#model-training",
    "title": "Regularization of Bayesian Neural Networks with Dropout Nodes",
    "section": "Model Training",
    "text": "Model Training\n\n\nEpoch 1/30\n800/800 - 1s - loss: 0.3835 - accuracy: 0.8947 - val_loss: 0.2465 - val_accuracy: 0.9010 - 718ms/epoch - 898us/step\nEpoch 2/30\n800/800 - 0s - loss: 0.2644 - accuracy: 0.9050 - val_loss: 0.1695 - val_accuracy: 0.9010 - 374ms/epoch - 467us/step\nEpoch 3/30\n800/800 - 0s - loss: 0.1931 - accuracy: 0.9118 - val_loss: 0.1397 - val_accuracy: 0.9600 - 360ms/epoch - 450us/step\nEpoch 4/30\n800/800 - 0s - loss: 0.1630 - accuracy: 0.9298 - val_loss: 0.1065 - val_accuracy: 0.9780 - 360ms/epoch - 450us/step\nEpoch 5/30\n800/800 - 0s - loss: 0.1545 - accuracy: 0.9340 - val_loss: 0.0983 - val_accuracy: 0.9720 - 363ms/epoch - 454us/step\nEpoch 6/30\n800/800 - 0s - loss: 0.1336 - accuracy: 0.9465 - val_loss: 0.0823 - val_accuracy: 0.9720 - 362ms/epoch - 452us/step\nEpoch 7/30\n800/800 - 0s - loss: 0.1284 - accuracy: 0.9500 - val_loss: 0.0640 - val_accuracy: 0.9850 - 362ms/epoch - 452us/step\nEpoch 8/30\n800/800 - 0s - loss: 0.1280 - accuracy: 0.9538 - val_loss: 0.0792 - val_accuracy: 0.9740 - 359ms/epoch - 449us/step\nEpoch 9/30\n800/800 - 0s - loss: 0.1203 - accuracy: 0.9563 - val_loss: 0.0691 - val_accuracy: 0.9840 - 361ms/epoch - 452us/step\nEpoch 10/30\n800/800 - 0s - loss: 0.1220 - accuracy: 0.9580 - val_loss: 0.0761 - val_accuracy: 0.9780 - 359ms/epoch - 449us/step\nEpoch 11/30\n800/800 - 0s - loss: 0.1143 - accuracy: 0.9615 - val_loss: 0.0757 - val_accuracy: 0.9810 - 360ms/epoch - 449us/step\nEpoch 12/30\n800/800 - 0s - loss: 0.1106 - accuracy: 0.9592 - val_loss: 0.0665 - val_accuracy: 0.9860 - 361ms/epoch - 452us/step\nEpoch 13/30\n800/800 - 0s - loss: 0.1155 - accuracy: 0.9610 - val_loss: 0.0732 - val_accuracy: 0.9790 - 358ms/epoch - 448us/step\nEpoch 14/30\n800/800 - 0s - loss: 0.1004 - accuracy: 0.9647 - val_loss: 0.0709 - val_accuracy: 0.9770 - 357ms/epoch - 446us/step\nEpoch 15/30\n800/800 - 0s - loss: 0.1028 - accuracy: 0.9607 - val_loss: 0.0793 - val_accuracy: 0.9840 - 361ms/epoch - 451us/step\nEpoch 16/30\n800/800 - 0s - loss: 0.1081 - accuracy: 0.9632 - val_loss: 0.0889 - val_accuracy: 0.9760 - 359ms/epoch - 449us/step\nEpoch 17/30\n800/800 - 0s - loss: 0.1037 - accuracy: 0.9668 - val_loss: 0.0703 - val_accuracy: 0.9850 - 358ms/epoch - 448us/step\nEpoch 18/30\n800/800 - 0s - loss: 0.0953 - accuracy: 0.9668 - val_loss: 0.0833 - val_accuracy: 0.9770 - 362ms/epoch - 453us/step\nEpoch 19/30\n800/800 - 0s - loss: 0.0989 - accuracy: 0.9668 - val_loss: 0.0822 - val_accuracy: 0.9800 - 360ms/epoch - 450us/step\nEpoch 20/30\n800/800 - 0s - loss: 0.1009 - accuracy: 0.9615 - val_loss: 0.0627 - val_accuracy: 0.9860 - 359ms/epoch - 448us/step\nEpoch 21/30\n800/800 - 0s - loss: 0.0942 - accuracy: 0.9650 - val_loss: 0.0827 - val_accuracy: 0.9790 - 357ms/epoch - 446us/step\nEpoch 22/30\n800/800 - 0s - loss: 0.0941 - accuracy: 0.9645 - val_loss: 0.0699 - val_accuracy: 0.9860 - 362ms/epoch - 453us/step\nEpoch 23/30\n800/800 - 0s - loss: 0.0913 - accuracy: 0.9653 - val_loss: 0.0669 - val_accuracy: 0.9840 - 358ms/epoch - 448us/step\nEpoch 24/30\n800/800 - 0s - loss: 0.0917 - accuracy: 0.9645 - val_loss: 0.0625 - val_accuracy: 0.9870 - 358ms/epoch - 447us/step\nEpoch 25/30\n800/800 - 0s - loss: 0.0931 - accuracy: 0.9630 - val_loss: 0.0666 - val_accuracy: 0.9880 - 362ms/epoch - 452us/step\nEpoch 26/30\n800/800 - 0s - loss: 0.0874 - accuracy: 0.9680 - val_loss: 0.0721 - val_accuracy: 0.9840 - 359ms/epoch - 449us/step\nEpoch 27/30\n800/800 - 0s - loss: 0.0940 - accuracy: 0.9660 - val_loss: 0.0578 - val_accuracy: 0.9870 - 359ms/epoch - 449us/step\nEpoch 28/30\n800/800 - 0s - loss: 0.0798 - accuracy: 0.9657 - val_loss: 0.0591 - val_accuracy: 0.9870 - 361ms/epoch - 451us/step\nEpoch 29/30\n800/800 - 0s - loss: 0.0834 - accuracy: 0.9705 - val_loss: 0.0777 - val_accuracy: 0.9800 - 357ms/epoch - 447us/step\nEpoch 30/30\n800/800 - 0s - loss: 0.0954 - accuracy: 0.9705 - val_loss: 0.0604 - val_accuracy: 0.9870 - 368ms/epoch - 461us/step"
  },
  {
    "objectID": "presentation.html#loss-and-validation-loss",
    "href": "presentation.html#loss-and-validation-loss",
    "title": "Regularization of Bayesian Neural Networks with Dropout Nodes",
    "section": "Loss and Validation Loss",
    "text": "Loss and Validation Loss"
  },
  {
    "objectID": "presentation.html#accuracy-and-validation-accuracy",
    "href": "presentation.html#accuracy-and-validation-accuracy",
    "title": "Regularization of Bayesian Neural Networks with Dropout Nodes",
    "section": "Accuracy and Validation Accuracy",
    "text": "Accuracy and Validation Accuracy"
  },
  {
    "objectID": "presentation.html#performance-evaluation",
    "href": "presentation.html#performance-evaluation",
    "title": "Regularization of Bayesian Neural Networks with Dropout Nodes",
    "section": "Performance Evaluation",
    "text": "Performance Evaluation\n\n\n32/32 - 0s - loss: 0.0604 - accuracy: 0.9870 - 53ms/epoch - 2ms/step\n\n\n      loss   accuracy \n0.06036564 0.98699999"
  },
  {
    "objectID": "presentation.html#insights-and-application",
    "href": "presentation.html#insights-and-application",
    "title": "Regularization of Bayesian Neural Networks with Dropout Nodes",
    "section": "Insights and Application",
    "text": "Insights and Application\n\n\n32/32 - 0s - 39ms/epoch - 1ms/step\n\n\n       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n  [1,]    0    0    0    0    0    0    0    0    0     0\n  [2,]    1    0    0    0    0    0    0    0    0     0\n  [3,]    0    0    0    0    0    0    0    0    0     0\n  [4,]    0    0    0    1    0    0    1    0    0     0\n  [5,]    0    0    0    0    1    0    0    0    0     0\n  [6,]    0    0    0    0    0    0    0    0    0     1\n  [7,]    0    0    0    0    0    0    0    0    0     0\n  [8,]    0    0    0    0    0    1    0    0    0     0\n  [9,]    1    0    0    1    0    0    0    0    0     0\n [10,]    0    0    0    0    0    0    0    0    0     0\n [11,]    0    0    1    0    0    0    0    0    0     0\n [12,]    0    0    0    0    0    1    0    0    0     0\n [13,]    0    1    0    0    0    0    0    0    0     0\n [14,]    0    0    0    0    1    0    0    0    0     0\n [15,]    0    1    0    0    0    0    0    0    0     0\n [16,]    0    0    0    1    0    0    0    0    1     0\n [17,]    0    1    0    0    0    0    0    0    0     0\n [18,]    0    0    0    0    1    0    0    0    0     0\n [19,]    0    1    0    0    0    0    0    0    0     0\n [20,]    0    0    0    0    0    0    0    0    0     0\n [21,]    0    0    0    0    0    0    0    0    1     0\n [22,]    0    0    0    0    0    0    0    0    0     0\n [23,]    0    0    1    0    0    1    0    0    1     0\n [24,]    1    0    0    0    0    0    0    0    0     0\n [25,]    0    0    1    0    1    0    0    0    0     0\n [26,]    0    0    0    0    0    0    0    0    0     0\n [27,]    0    0    0    0    0    0    0    0    0     0\n [28,]    0    0    0    0    0    0    0    0    0     0\n [29,]    0    0    1    0    0    0    0    0    0     1\n [30,]    0    0    1    0    1    0    0    0    1     0\n [31,]    0    1    0    1    1    0    0    0    0     0\n [32,]    0    0    0    0    0    0    0    0    0     0\n [33,]    1    0    0    0    0    0    1    0    0     0\n [34,]    0    0    0    0    1    0    0    0    0     0\n [35,]    1    0    0    0    0    1    0    0    0     0\n [36,]    0    0    0    0    0    0    0    0    1     0\n [37,]    0    0    0    0    0    0    1    1    0     0\n [38,]    0    0    0    0    0    0    0    0    0     0\n [39,]    1    0    0    1    0    0    0    0    0     0\n [40,]    0    0    0    0    0    0    0    0    0     0\n [41,]    1    0    0    0    0    0    0    0    1     0\n [42,]    0    0    0    1    0    0    0    1    0     1\n [43,]    0    0    0    0    0    0    0    0    0     0\n [44,]    1    0    0    0    0    0    0    0    0     0\n [45,]    1    0    0    0    1    0    1    0    0     0\n [46,]    1    0    0    0    0    0    0    0    0     0\n [47,]    0    0    1    0    0    0    0    0    0     0\n [48,]    0    0    0    0    0    0    0    0    0     0\n [49,]    0    1    0    1    0    0    0    0    0     0\n [50,]    0    0    0    0    0    0    0    0    0     0\n [51,]    0    0    0    1    0    0    0    0    0     1\n [52,]    0    0    0    0    0    0    0    0    0     0\n [53,]    0    0    0    0    0    0    0    0    0     0\n [54,]    0    0    0    0    0    0    0    0    0     0\n [55,]    1    0    0    1    0    0    0    0    0     1\n [56,]    0    0    0    0    0    0    0    0    0     0\n [57,]    0    0    1    0    1    0    0    0    0     0\n [58,]    0    0    0    0    0    0    1    0    0     0\n [59,]    0    0    1    0    0    0    0    0    0     0\n [60,]    0    0    0    0    0    0    0    0    0     0\n [61,]    0    0    1    0    0    0    0    0    0     0\n [62,]    0    0    1    0    0    0    0    0    0     0\n [63,]    0    0    0    0    0    0    0    0    0     0\n [64,]    0    0    0    1    0    0    0    0    0     0\n [65,]    0    0    0    0    1    0    0    0    0     1\n [66,]    0    0    0    0    0    0    0    0    1     0\n [67,]    0    0    0    0    0    0    0    0    0     0\n [68,]    0    0    0    0    0    0    0    0    0     0\n [69,]    0    0    0    0    0    1    0    1    0     0\n [70,]    0    0    0    0    0    0    0    0    0     0\n [71,]    0    0    0    0    0    0    0    0    0     0\n [72,]    0    0    1    0    0    1    0    0    0     0\n [73,]    0    0    1    0    0    0    0    0    0     0\n [74,]    0    0    0    0    0    0    0    0    0     0\n [75,]    0    0    0    0    0    0    0    0    0     0\n [76,]    0    0    0    0    0    0    0    0    0     0\n [77,]    0    0    0    0    0    0    0    0    0     0\n [78,]    0    1    0    0    0    0    0    0    0     0\n [79,]    0    0    0    0    1    0    0    0    1     0\n [80,]    0    0    0    0    1    0    0    0    0     0\n [81,]    0    0    0    0    0    0    1    0    0     0\n [82,]    0    0    0    0    0    0    0    0    0     0\n [83,]    0    0    1    0    0    0    0    0    0     1\n [84,]    0    0    0    0    1    0    0    0    0     0\n [85,]    0    0    1    0    0    0    0    0    0     0\n [86,]    0    0    0    0    0    0    1    0    0     0\n [87,]    0    0    0    0    1    0    0    0    1     0\n [88,]    0    0    0    0    0    0    0    0    0     0\n [89,]    0    0    0    0    0    0    0    0    0     0\n [90,]    1    0    0    0    0    0    0    0    0     0\n [91,]    0    0    0    0    0    0    0    0    0     0\n [92,]    0    0    0    0    0    0    0    0    0     1\n [93,]    0    0    0    0    0    0    0    0    0     0\n [94,]    0    0    1    0    0    0    0    1    0     0\n [95,]    0    0    0    0    0    0    0    0    1     0\n [96,]    0    0    0    0    0    0    0    1    0     0\n [97,]    1    0    0    0    0    0    0    0    0     0\n [98,]    0    0    0    0    0    1    0    0    0     0\n [99,]    0    0    0    1    0    0    0    0    0     0\n[100,]    0    0    0    1    0    0    0    0    0     0\n\n\nprop_of_loan_approvals\n  0   1 \n902  98"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regularization of Bayesian NN by Dropout Technique",
    "section": "",
    "text": "This project aims to develop a Bayesian Neural Network with Monte Carlo Dropout to predict whether a loan applicant will be approved for a personal loan, using a dataset of 5,000 samples. The dataset includes 12 variables representing various aspects of the applicant’s financial profile, including age, income, credit score, and account types. The model will incorporate dropout regularization to prevent overfitting and improve generalization. The goal is to evaluate the model’s ability to accurately predict loan approval and assess how well Monte Carlo Dropout regulates the model, preventing overfitting and enhancing prediction accuracy. This approach offers insights into the potential of BNNs in financial decision-making and the effectiveness of dropout techniques in improving model performance."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Regularization of Bayesian NN by Dropout Technique",
    "section": "",
    "text": "This project aims to develop a Bayesian Neural Network with Monte Carlo Dropout to predict whether a loan applicant will be approved for a personal loan, using a dataset of 5,000 samples. The dataset includes 12 variables representing various aspects of the applicant’s financial profile, including age, income, credit score, and account types. The model will incorporate dropout regularization to prevent overfitting and improve generalization. The goal is to evaluate the model’s ability to accurately predict loan approval and assess how well Monte Carlo Dropout regulates the model, preventing overfitting and enhancing prediction accuracy. This approach offers insights into the potential of BNNs in financial decision-making and the effectiveness of dropout techniques in improving model performance."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Regularization of Bayesian NN by Dropout Technique",
    "section": "Introduction",
    "text": "Introduction\nThe ability to accurately predict loan approval is a critical aspect of financial decision-making. This project aims to develop a Bayesian Neural Network (BNN) to predict whether a loan applicant will be approved for a personal loan, based on a variety of financial and demographic factors. We will apply Monte Carlo Dropout regularization to enhance the model’s generalization, prevent overfitting, and provide uncertainty estimates in predictions.\nUsing a dataset of 5,000 loan applicants, with 12 variables such as age, income, credit score, and account types, the model will assist financial institutions in making informed and data-driven decisions regarding loan approvals. The application of BNNs and Monte Carlo Dropout techniques aims to improve prediction accuracy and offer valuable insights into the financial decision-making process."
  },
  {
    "objectID": "index.html#dataset-description",
    "href": "index.html#dataset-description",
    "title": "Regularization of Bayesian NN by Dropout Technique",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe dataset used in this project is the Bank Loan Approval dataset from Kaggle (2024). It contains 5,000 stratified samples of loan applicants, with various financial and demographic features. This dataset is ideal for predicting whether an applicant will be approved for a personal loan based on multiple variables, including income, credit score, and other financial indicators.\n\nKey Variables:\n\nAge – Age of the loan applicant (23 - 67 years).\nYears of Work Experience – Total years of work experience (0 - 43 years).\nIncome – Annual income of the applicant ($8,000 - $224,000).\nNumber of Family Members – Number of people in the applicant’s family (1 - 4 members).\nAverage Credit Card Score – Credit score on a scale of 0 to 10.\nEducation Level – Education level of the applicant (1 - 3, with 1 being least educated).\nValue of Home Mortgage – Mortgage value of the applicant’s home ($0 - $635,000).\nPersonal Loan Approved – Binary outcome indicating if the personal loan was approved (0 = No, 1 = Yes) (response variable).\nHas a Securities Account – Whether the applicant has a securities account (0 = No, 1 = Yes).\nHas a CD Account – Whether the applicant has a certificate of deposit (0 = No, 1 = Yes).\nHas an Online Banking Account – Whether the applicant has an online banking account (0 = No, 1 = Yes).\nHas a Credit Card – Whether the applicant has a credit card (0 = No, 1 = Yes).\n\n\n\nPurpose:\nThese variables provide a comprehensive view of the factors that influence loan approval decisions. By using this dataset, we aim to build a predictive model that helps financial institutions assess the likelihood of loan approval based on an applicant’s financial and demographic information. The goal is to automate and optimize the loan approval process, enabling faster, data-driven decisions that improve operational efficiency and reduce risk for lenders."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Regularization of Bayesian NN by Dropout Technique",
    "section": "Research Questions",
    "text": "Research Questions\nThe primary goal of this project is to explore the effectiveness of predicting loan approval decisions using machine learning techniques. Specifically, we aim to address the following research questions:\n\nHow effectively can a neural network predict if a loan applicant will be approved or denied for a personal loan?\nThis question focuses on evaluating the ability of a neural network to learn from various features, such as income, credit score, and years of work experience, to predict loan approval decisions.\nCan the Monte Carlo Dropout technique effectively regulate a neural network and keep it from overfitting the training data?\nThis question investigates the role of Monte Carlo Dropout in enhancing model performance by reducing overfitting, improving generalization, and providing uncertainty estimates for predictions, ensuring that the neural network does not memorize the training data and can generalize better to unseen loan application data."
  },
  {
    "objectID": "index.html#approach-overview",
    "href": "index.html#approach-overview",
    "title": "Regularization of Bayesian NN by Dropout Technique",
    "section": "Approach Overview",
    "text": "Approach Overview\nTo predict loan approval decisions using a Bayesian Neural Network (BNN) with Monte Carlo Dropout, we will follow a structured approach involving data preprocessing, model development, and performance evaluation. The goal is to train a neural network to classify whether a loan applicant will be approved or denied, leveraging a stratified dataset with various demographic and financial features.\n\n1. Data Preprocessing\n\nData Cleaning: Handle missing values, outliers, and ensure consistency across variables to ensure the dataset is accurate and ready for modeling.\nFeature Engineering: Create additional features where necessary, such as aggregating work hours or categorizing mental health-related conditions to provide more relevant insights for the model.\nNormalization: Normalize numerical features to improve model convergence, stability, and ensure that features on different scales do not negatively impact performance.\nEncoding: Convert categorical variables into appropriate formats (e.g., one-hot encoding or label encoding) to ensure the model can process them effectively.\n\n\n      Age           Experience         Income           Family     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :1.000  \n 1st Qu.:0.2727   1st Qu.:0.2826   1st Qu.:0.1435   1st Qu.:1.000  \n Median :0.5000   Median :0.5000   Median :0.2593   Median :2.000  \n Mean   :0.5077   Mean   :0.5023   Mean   :0.3045   Mean   :2.396  \n 3rd Qu.:0.7273   3rd Qu.:0.7174   3rd Qu.:0.4167   3rd Qu.:3.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :4.000  \n     CCAvg          Education        Mortgage       Personal.Loan  \n Min.   :0.0000   Min.   :1.000   Min.   :0.00000   Min.   :0.000  \n 1st Qu.:0.0700   1st Qu.:1.000   1st Qu.:0.00000   1st Qu.:0.000  \n Median :0.1500   Median :2.000   Median :0.00000   Median :0.000  \n Mean   :0.1938   Mean   :1.881   Mean   :0.08897   Mean   :0.096  \n 3rd Qu.:0.2500   3rd Qu.:3.000   3rd Qu.:0.15906   3rd Qu.:0.000  \n Max.   :1.0000   Max.   :3.000   Max.   :1.00000   Max.   :1.000  \n Securities.Account   CD.Account         Online         CreditCard   \n Min.   :0.0000     Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000     1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000     Median :0.0000   Median :1.0000   Median :0.000  \n Mean   :0.1044     Mean   :0.0604   Mean   :0.5968   Mean   :0.294  \n 3rd Qu.:0.0000     3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000     Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n\n\n\n\n\n2. Model Development\n\nNeural Network Architecture: Develop a BNN with 4 hidden layers. The network consists of the following layers:\n\nLayer 1: 128 neurons with ReLU activation\nLayer 2: 64 neurons with ReLU activation\nLayer 3: 32 neurons with ReLU activation\nLayer 4: 16 neurons with ReLU activation\nOutput Layer: 1 neuron with Sigmoid activation for binary classification (loan approval: 0 or 1).\n\nThis architecture is chosen to balance model complexity and training efficiency while ensuring that the model can capture non-linear relationships in the data.\nMonte Carlo Dropout: Implement Monte Carlo Dropout during both training and inference. The dropout technique randomly drops a percentage of neurons during training to prevent overfitting.\nLoss Function & Optimization: Use binary cross-entropy loss since the task is a binary classification (loan approval or denial). To minimize the loss function, employ the Adam optimizer, which is well-suited for training deep neural networks and adjusts the learning rate during training for efficient convergence.\n\n\n\n3. Model Training\n\nData Split: Split the dataset into an 80/20 train-test split to evaluate model performance.\nHyperparameter Tuning: Tune key hyperparameters such as the number of layers, neurons, dropout rate, and learning rate using k-fold cross-validation.\nNeural Network with MC Dropout\n\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n dense_4 (Dense)                    (None, 128)                     1536        \n dropout_3 (Dropout)                (None, 128)                     0           \n dense_3 (Dense)                    (None, 64)                      8256        \n dropout_2 (Dropout)                (None, 64)                      0           \n dense_2 (Dense)                    (None, 32)                      2080        \n dropout_1 (Dropout)                (None, 32)                      0           \n dense_1 (Dense)                    (None, 16)                      528         \n dropout (Dropout)                  (None, 16)                      0           \n dense (Dense)                      (None, 1)                       17          \n================================================================================\nTotal params: 12417 (48.50 KB)\nTrainable params: 12417 (48.50 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\nModel Training\n\n\nEpoch 1/30\n800/800 - 1s - loss: 0.4126 - accuracy: 0.8823 - val_loss: 0.2465 - val_accuracy: 0.9010 - 714ms/epoch - 893us/step\nEpoch 2/30\n800/800 - 0s - loss: 0.2566 - accuracy: 0.9043 - val_loss: 0.1756 - val_accuracy: 0.9010 - 375ms/epoch - 469us/step\nEpoch 3/30\n800/800 - 0s - loss: 0.2050 - accuracy: 0.9143 - val_loss: 0.1261 - val_accuracy: 0.9510 - 433ms/epoch - 541us/step\nEpoch 4/30\n800/800 - 0s - loss: 0.1724 - accuracy: 0.9330 - val_loss: 0.1074 - val_accuracy: 0.9640 - 364ms/epoch - 455us/step\nEpoch 5/30\n800/800 - 0s - loss: 0.1386 - accuracy: 0.9477 - val_loss: 0.0845 - val_accuracy: 0.9780 - 367ms/epoch - 459us/step\nEpoch 6/30\n800/800 - 0s - loss: 0.1389 - accuracy: 0.9503 - val_loss: 0.0875 - val_accuracy: 0.9690 - 369ms/epoch - 461us/step\nEpoch 7/30\n800/800 - 0s - loss: 0.1208 - accuracy: 0.9592 - val_loss: 0.0703 - val_accuracy: 0.9760 - 366ms/epoch - 457us/step\nEpoch 8/30\n800/800 - 0s - loss: 0.1269 - accuracy: 0.9625 - val_loss: 0.0729 - val_accuracy: 0.9790 - 364ms/epoch - 455us/step\nEpoch 9/30\n800/800 - 0s - loss: 0.1221 - accuracy: 0.9597 - val_loss: 0.0659 - val_accuracy: 0.9810 - 366ms/epoch - 457us/step\nEpoch 10/30\n800/800 - 0s - loss: 0.1018 - accuracy: 0.9628 - val_loss: 0.0560 - val_accuracy: 0.9840 - 363ms/epoch - 453us/step\nEpoch 11/30\n800/800 - 0s - loss: 0.0947 - accuracy: 0.9715 - val_loss: 0.0712 - val_accuracy: 0.9730 - 363ms/epoch - 453us/step\nEpoch 12/30\n800/800 - 0s - loss: 0.1104 - accuracy: 0.9663 - val_loss: 0.0956 - val_accuracy: 0.9650 - 361ms/epoch - 452us/step\nEpoch 13/30\n800/800 - 0s - loss: 0.1073 - accuracy: 0.9685 - val_loss: 0.0512 - val_accuracy: 0.9870 - 364ms/epoch - 455us/step\nEpoch 14/30\n800/800 - 0s - loss: 0.0986 - accuracy: 0.9693 - val_loss: 0.0647 - val_accuracy: 0.9780 - 362ms/epoch - 453us/step\nEpoch 15/30\n800/800 - 0s - loss: 0.1044 - accuracy: 0.9672 - val_loss: 0.0640 - val_accuracy: 0.9800 - 362ms/epoch - 453us/step\nEpoch 16/30\n800/800 - 0s - loss: 0.0975 - accuracy: 0.9695 - val_loss: 0.0706 - val_accuracy: 0.9780 - 363ms/epoch - 454us/step\nEpoch 17/30\n800/800 - 0s - loss: 0.0900 - accuracy: 0.9697 - val_loss: 0.0871 - val_accuracy: 0.9730 - 360ms/epoch - 450us/step\nEpoch 18/30\n800/800 - 0s - loss: 0.0903 - accuracy: 0.9732 - val_loss: 0.0627 - val_accuracy: 0.9790 - 362ms/epoch - 453us/step\nEpoch 19/30\n800/800 - 0s - loss: 0.0916 - accuracy: 0.9715 - val_loss: 0.0648 - val_accuracy: 0.9800 - 361ms/epoch - 452us/step\nEpoch 20/30\n800/800 - 0s - loss: 0.0941 - accuracy: 0.9718 - val_loss: 0.0599 - val_accuracy: 0.9850 - 363ms/epoch - 453us/step\nEpoch 21/30\n800/800 - 0s - loss: 0.0931 - accuracy: 0.9728 - val_loss: 0.0651 - val_accuracy: 0.9770 - 363ms/epoch - 454us/step\nEpoch 22/30\n800/800 - 0s - loss: 0.0926 - accuracy: 0.9705 - val_loss: 0.0536 - val_accuracy: 0.9850 - 367ms/epoch - 459us/step\nEpoch 23/30\n800/800 - 0s - loss: 0.0857 - accuracy: 0.9753 - val_loss: 0.0606 - val_accuracy: 0.9830 - 366ms/epoch - 457us/step\nEpoch 24/30\n800/800 - 0s - loss: 0.0766 - accuracy: 0.9762 - val_loss: 0.0595 - val_accuracy: 0.9850 - 362ms/epoch - 452us/step\nEpoch 25/30\n800/800 - 0s - loss: 0.0816 - accuracy: 0.9750 - val_loss: 0.0660 - val_accuracy: 0.9780 - 365ms/epoch - 456us/step\nEpoch 26/30\n800/800 - 0s - loss: 0.0971 - accuracy: 0.9670 - val_loss: 0.0618 - val_accuracy: 0.9860 - 364ms/epoch - 455us/step\nEpoch 27/30\n800/800 - 0s - loss: 0.0853 - accuracy: 0.9715 - val_loss: 0.0554 - val_accuracy: 0.9870 - 361ms/epoch - 451us/step\nEpoch 28/30\n800/800 - 0s - loss: 0.0866 - accuracy: 0.9730 - val_loss: 0.0611 - val_accuracy: 0.9850 - 361ms/epoch - 451us/step\nEpoch 29/30\n800/800 - 0s - loss: 0.0844 - accuracy: 0.9755 - val_loss: 0.0602 - val_accuracy: 0.9840 - 364ms/epoch - 456us/step\nEpoch 30/30\n800/800 - 0s - loss: 0.0960 - accuracy: 0.9690 - val_loss: 0.0663 - val_accuracy: 0.9790 - 363ms/epoch - 454us/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Performance Evaluation\n\nAccuracy: Measure model performance using accuracy, precision, recall, and F1-score for each stress level category (low, medium, high).\nUncertainty Estimation: Assess the uncertainty in predictions using Monte Carlo Dropout by averaging over multiple forward passes.\nModel Interpretability: Explore which features contribute most to the model’s predictions using techniques like SHAP values.\n\n\n32/32 - 0s - loss: 0.0663 - accuracy: 0.9790 - 55ms/epoch - 2ms/step\n\n\n      loss   accuracy \n0.06631203 0.97899997 \n\n\n\n\n\n5. Insights and Application\n\nA BNN model has a high degree of accuracy in predicting classifations labels and is a good candidate to use alongside other classification models, such as Naive Bayes, Gradient Boosting, Support Vector Machines, etc.\nIn the case of this dataset, financial institutions and loan originators can use a BNN model as supplemental analysis in their loan approval process, or use it to forecast changes in a loan porfolio. Likewise, consumers can use a BNN to analyze their chances of being approved for a loan and adjust variables on their end.\n\n\n32/32 - 0s - 41ms/epoch - 1ms/step\n\n\n       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n  [1,]    0    0    0    0    0    0    0    0    0     0\n  [2,]    1    0    0    0    0    0    0    0    0     0\n  [3,]    0    0    0    0    0    0    0    0    0     0\n  [4,]    0    0    0    1    0    0    0    0    0     0\n  [5,]    0    0    0    0    1    0    0    0    0     0\n  [6,]    0    0    0    0    0    0    0    0    0     1\n  [7,]    0    0    0    0    0    0    0    0    0     0\n  [8,]    0    0    0    0    0    1    0    0    0     0\n  [9,]    1    0    0    0    0    0    0    0    0     0\n [10,]    0    0    0    0    0    0    0    0    0     0\n [11,]    0    0    1    0    0    0    0    0    0     0\n [12,]    0    0    0    0    0    0    0    0    0     0\n [13,]    0    1    0    0    0    0    0    0    0     0\n [14,]    0    0    0    0    1    0    0    0    0     0\n [15,]    0    1    0    0    0    0    0    0    0     0\n [16,]    0    0    0    1    0    0    0    0    1     0\n [17,]    0    1    0    0    0    0    0    0    0     0\n [18,]    0    0    0    0    1    0    0    0    0     0\n [19,]    0    1    0    0    0    0    0    0    0     0\n [20,]    0    0    0    0    0    0    0    0    0     0\n [21,]    0    0    0    0    0    0    0    0    0     0\n [22,]    0    0    0    0    0    0    0    0    0     0\n [23,]    0    0    1    0    0    1    0    0    1     0\n [24,]    1    0    0    0    0    0    0    0    0     0\n [25,]    0    0    1    0    1    0    0    0    0     0\n [26,]    0    0    0    0    0    0    0    0    0     0\n [27,]    0    0    0    0    0    0    0    0    0     0\n [28,]    0    0    0    0    0    0    0    0    0     0\n [29,]    0    0    1    0    0    0    0    0    0     0\n [30,]    0    0    1    0    1    0    0    0    1     0\n [31,]    0    1    0    1    0    0    0    0    0     0\n [32,]    0    0    0    0    0    0    0    0    0     0\n [33,]    1    0    0    0    0    0    1    0    0     0\n [34,]    0    0    0    0    1    0    0    0    0     0\n [35,]    1    0    0    0    0    0    0    0    0     0\n [36,]    0    0    0    0    0    0    0    0    1     0\n [37,]    0    0    0    0    0    0    1    1    0     0\n [38,]    0    0    0    0    0    0    0    0    0     0\n [39,]    1    0    0    1    0    0    0    0    0     0\n [40,]    0    0    0    0    0    0    0    0    0     0\n [41,]    1    0    0    0    0    0    0    0    1     0\n [42,]    0    0    0    1    0    0    0    1    0     1\n [43,]    0    0    0    0    0    0    0    0    0     0\n [44,]    1    0    0    0    0    0    0    0    0     0\n [45,]    1    0    0    0    1    0    1    0    0     0\n [46,]    1    0    0    0    0    0    0    0    0     0\n [47,]    0    0    0    0    0    0    0    0    0     0\n [48,]    0    0    0    0    0    0    0    0    0     0\n [49,]    0    0    0    1    0    0    0    0    0     0\n [50,]    0    0    0    0    0    0    0    0    0     0\n [51,]    0    0    0    1    0    0    0    0    0     0\n [52,]    0    0    0    0    0    0    0    0    0     0\n [53,]    0    0    0    0    0    0    0    0    0     0\n [54,]    0    0    0    0    0    0    0    0    0     0\n [55,]    1    0    0    1    0    0    0    0    0     1\n [56,]    0    0    0    0    0    0    0    0    0     0\n [57,]    0    0    1    0    1    0    0    0    0     0\n [58,]    0    0    0    0    0    0    1    0    0     0\n [59,]    0    0    1    0    0    0    0    0    0     0\n [60,]    0    0    0    0    0    0    0    0    0     0\n [61,]    0    0    1    0    0    0    0    0    0     0\n [62,]    0    0    1    0    0    0    0    0    0     0\n [63,]    0    0    0    0    0    0    0    0    0     0\n [64,]    0    0    0    1    0    0    0    0    0     0\n [65,]    0    0    0    0    1    0    0    0    0     1\n [66,]    0    0    0    0    0    0    0    0    1     0\n [67,]    0    0    0    0    0    0    0    0    0     0\n [68,]    0    0    0    0    0    0    0    0    0     0\n [69,]    0    0    0    0    0    1    0    0    0     0\n [70,]    0    0    0    0    0    0    0    0    0     0\n [71,]    0    0    0    0    0    0    0    0    0     0\n [72,]    0    0    1    0    0    1    0    0    0     0\n [73,]    0    0    1    0    0    0    0    0    0     0\n [74,]    0    0    0    0    0    0    0    0    0     0\n [75,]    0    0    0    0    0    0    0    0    0     0\n [76,]    0    0    0    0    0    0    0    0    0     0\n [77,]    0    0    0    0    0    0    0    0    0     0\n [78,]    0    1    0    0    0    0    0    0    0     0\n [79,]    0    0    0    0    1    0    0    0    1     0\n [80,]    0    0    0    0    0    0    0    0    0     0\n [81,]    0    0    0    0    0    0    1    0    0     0\n [82,]    0    0    0    0    0    0    0    0    0     0\n [83,]    0    0    1    0    0    0    0    0    0     1\n [84,]    0    0    0    0    1    0    0    0    0     0\n [85,]    0    0    1    0    0    0    0    0    0     0\n [86,]    0    0    0    0    0    0    1    0    0     0\n [87,]    0    0    0    0    1    0    0    0    1     0\n [88,]    0    0    0    0    0    0    0    0    0     0\n [89,]    0    0    0    0    0    0    0    0    0     0\n [90,]    1    0    0    0    0    0    0    0    0     0\n [91,]    0    0    0    0    0    0    0    0    0     0\n [92,]    0    0    0    0    0    0    0    0    0     1\n [93,]    0    0    0    0    0    0    0    0    0     0\n [94,]    0    0    0    0    0    0    0    1    0     0\n [95,]    0    0    0    0    0    0    0    0    1     0\n [96,]    0    0    0    0    0    0    0    1    0     0\n [97,]    1    0    0    0    0    0    0    0    0     0\n [98,]    0    0    0    0    0    1    0    0    0     0\n [99,]    0    0    0    0    0    0    0    0    0     0\n[100,]    0    0    0    1    0    0    0    0    0     0\n\n\nprop_of_loan_approvals\n  0   1 \n916  84"
  },
  {
    "objectID": "index.html#model-architecture",
    "href": "index.html#model-architecture",
    "title": "Regularization of Bayesian NN by Dropout Technique",
    "section": "Model Architecture:",
    "text": "Model Architecture:\n\nInput Layer: Takes features from x_train.\nHidden Layers: Four layers with 128, 64, 32, and 16 neurons, using ReLU activation.\nDropout: Dropout applied after each layer with rates 0.3, 0.4, 0.5, and 0.6 to prevent overfitting. Monte Carlo Dropout keeps dropout active during inference for uncertainty estimation.\n\n\nOutput Layer:\n\nSoftmax activation for predicting three classes: low, medium, and high stress.\n\n\n\nTraining:\n\nLoss: Categorical cross-entropy for multi-class classification.\nOptimizer: Adam.\nEpochs: 50 with batch size 5."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Regularization of Bayesian NN by Dropout Technique",
    "section": "Conclusion:",
    "text": "Conclusion:\nWe successfully produced a BNN that accurately predicts classification labels for a multi-variable dataset and keep said model from overfitting the data by utilizing the MC Dropout technique.\nWhile our model was used specifically to determine if a loan applicant would be approved for a personal loan, it can also be used to determine if an applicant will be approved for any type of loan (car, mortgage, business etc.) or applied to any field that has a sufficiently-sampled, correlated dataset, and still provide a high degree of accuracy in predicting classification labels."
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Regularization of Bayesian NN",
    "section": "",
    "text": "The goal of this project is to develop a Bayesian Neural Network, enhanced with the Monte Carlo Dropout regularization technique, to accurately predict the likelihood of a loan applicant being approved or denied. The model will be trained on a dataset containing applicant metadata—such as age, income, credit score, years of experience, and mortgage value—and will predict whether the applicant is likely to be approved for a loan. By using dropout during both training and inference, we aim to improve the model’s generalization and estimate prediction uncertainty, ultimately providing a more reliable decision-making tool for financial institutions."
  },
  {
    "objectID": "proposal.html#project-goal",
    "href": "proposal.html#project-goal",
    "title": "Regularization of Bayesian NN",
    "section": "",
    "text": "The goal of this project is to develop a Bayesian Neural Network, enhanced with the Monte Carlo Dropout regularization technique, to accurately predict the likelihood of a loan applicant being approved or denied. The model will be trained on a dataset containing applicant metadata—such as age, income, credit score, years of experience, and mortgage value—and will predict whether the applicant is likely to be approved for a loan. By using dropout during both training and inference, we aim to improve the model’s generalization and estimate prediction uncertainty, ultimately providing a more reliable decision-making tool for financial institutions."
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "Regularization of Bayesian NN",
    "section": "Questions",
    "text": "Questions\n\nHow effectively can a neural network predict whether a loan applicant will be approved or denied based on individual financial and demographic metadata?\nHow does the Monte Carlo Dropout technique impact the performance and generalization of a neural network in predicting loan approval outcomes?"
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Regularization of Bayesian NN",
    "section": "Dataset",
    "text": "Dataset\n\n# Load the necessary libraries\nlibrary(readr)\nlibrary(dplyr)\n\n# Load the data\ndf &lt;- read_csv(\"data/bankloan.csv\")\n\n# Review all predictors\nsummary(df)\n\n      Age          Experience       Income           Family     \n Min.   :23.00   Min.   :-3.0   Min.   :  8.00   Min.   :1.000  \n 1st Qu.:35.00   1st Qu.:10.0   1st Qu.: 39.00   1st Qu.:1.000  \n Median :45.00   Median :20.0   Median : 64.00   Median :2.000  \n Mean   :45.34   Mean   :20.1   Mean   : 73.77   Mean   :2.396  \n 3rd Qu.:55.00   3rd Qu.:30.0   3rd Qu.: 98.00   3rd Qu.:3.000  \n Max.   :67.00   Max.   :43.0   Max.   :224.00   Max.   :4.000  \n     CCAvg          Education        Mortgage     Personal.Loan  \n Min.   : 0.000   Min.   :1.000   Min.   :  0.0   Min.   :0.000  \n 1st Qu.: 0.700   1st Qu.:1.000   1st Qu.:  0.0   1st Qu.:0.000  \n Median : 1.500   Median :2.000   Median :  0.0   Median :0.000  \n Mean   : 1.938   Mean   :1.881   Mean   : 56.5   Mean   :0.096  \n 3rd Qu.: 2.500   3rd Qu.:3.000   3rd Qu.:101.0   3rd Qu.:0.000  \n Max.   :10.000   Max.   :3.000   Max.   :635.0   Max.   :1.000  \n Securities.Account   CD.Account         Online         CreditCard   \n Min.   :0.0000     Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000     1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000     Median :0.0000   Median :1.0000   Median :0.000  \n Mean   :0.1044     Mean   :0.0604   Mean   :0.5968   Mean   :0.294  \n 3rd Qu.:0.0000     3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000     Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n\n# Analyze Mortgage predictor\n# View mortgage $ data\n#range(df$Mortgage) # $0-$635\n\n# Count the number of values greater than 0 in the Mortgage column\n#count_above_zero &lt;- sum(df$Mortgage &gt; 0, na.rm = TRUE)\n\n# Get the total number of non-missing values in the column\n#total_values &lt;- sum(!is.na(df$Mortgage))\n\n# Calculate the proportion\n#proportion_above_zero &lt;- count_above_zero / total_values\n\n# Print the results\n#count_above_zero # 1,538 ppl with mortgages\n#proportion_above_zero # 30.76% of people have loans\n\n# Remove Mortgage\n#df &lt;- select(df, -\"Mortgage\")\n\n# Normalize using Min-Max Scaling\n# df &lt;- df %&gt;%\n#   mutate(\n#     Age = (Age - min(Age)) / (max(Age) - min(Age)),\n#     Experience = (Experience - min(Experience)) / (max(Experience) - min(Experience)),\n#     Income = (Income - min(Income)) / (max(Income) - min(Income)),\n#     CCAvg = (CCAvg - min(CCAvg)) / (max(CCAvg) - min(CCAvg)),\n#     Mortgage = (Mortgage - min(Mortgage, na.rm = TRUE)) / (max(Mortgage, na.rm = TRUE) - min(Mortgage, na.rm = TRUE))\n#   )\n# \n# summary(df)\n\n\nDataset Description\nThis dataset consists of 5,000 loan applicants with 12 variables that capture demographic, financial, and credit-related factors. It includes information such as age, income, family size, credit score, employment status, and whether the applicant has certain financial accounts (e.g., a credit card or online banking). The target variable indicates whether a personal loan was approved (1) or denied (0).\n\nDimensions:\n\nRows: 5000 (employees)\nColumns: 12 variables that capture demographic, financial, etc.\n\n\n\nKey Variables:\n\nNumerical: Age, Income, Years of Experience, Credit Score, Mortgage, etc.\nCategorical: Gender, Job Role, Family Size, Education, Personal Loan Status, etc.\n\n\n\nProvenance:\nThe dataset is synthetic, designed for research on employee wellbeing, workplace dynamics, and productivity, incorporating common variables seen in employee surveys.\n\n\nReason for Choosing:\nIt’s relevant for analyzing the impact of work environments (remote, hybrid, onsite) on mental health, work-life balance, and productivity, making it valuable for HR and organizational studies."
  },
  {
    "objectID": "proposal.html#priors---parameters-hyperparameters",
    "href": "proposal.html#priors---parameters-hyperparameters",
    "title": "Regularization of Bayesian NN",
    "section": "Priors - Parameters & Hyperparameters:",
    "text": "Priors - Parameters & Hyperparameters:\nFor the model, Gaussian distributions are used for priors, aligning with the smoothness and differentiability needed for effective optimization. The key features influencing stress levels, such as age, job role, work location, mental health condition, and hours worked, will determine the priors.\nParameter Initialization:\n\nWeights (W_ij): The weights will be initialized using a Gaussian distribution with: Wij∼N(0,0.5)Wij​∼N(0,0.5)\nwhere μ = 0 and σ = 0.5.\nBiases (b_j): The biases will have a smaller variance to remain near zero: bj∼N(0,0.1)bj​∼N(0,0.1)\nwhere μ = 0 and σ = 0.1.\n\nHyperparameters:\n\nModel Architecture: 4 hidden layers with 128, 64, 32, and 16 units, respectively.\nActivation Functions: ReLU for hidden layers, Sigmoid for output layer.\nTraining Configuration:\n\nTraining data: 80%, Testing data: 20%\nEpochs: 30\nBatch Size: 5\nLoss Function: Binary Cross-Entropy\n\nOptimization: Adam optimizer, known for its efficiency in handling complex, non-convex optimization problems like neural network training."
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Regularization of Bayesian Neural Networks with Dropout Nodes",
    "section": "",
    "text": "Rows: 5000 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (12): Age, Experience, Income, Family, CCAvg, Education, Mortgage, Perso...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n      Age           Experience         Income           Family     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :1.000  \n 1st Qu.:0.2727   1st Qu.:0.2826   1st Qu.:0.1435   1st Qu.:1.000  \n Median :0.5000   Median :0.5000   Median :0.2593   Median :2.000  \n Mean   :0.5077   Mean   :0.5023   Mean   :0.3045   Mean   :2.396  \n 3rd Qu.:0.7273   3rd Qu.:0.7174   3rd Qu.:0.4167   3rd Qu.:3.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :4.000  \n     CCAvg          Education        Mortgage       Personal.Loan  \n Min.   :0.0000   Min.   :1.000   Min.   :0.00000   Min.   :0.000  \n 1st Qu.:0.0700   1st Qu.:1.000   1st Qu.:0.00000   1st Qu.:0.000  \n Median :0.1500   Median :2.000   Median :0.00000   Median :0.000  \n Mean   :0.1938   Mean   :1.881   Mean   :0.08897   Mean   :0.096  \n 3rd Qu.:0.2500   3rd Qu.:3.000   3rd Qu.:0.15906   3rd Qu.:0.000  \n Max.   :1.0000   Max.   :3.000   Max.   :1.00000   Max.   :1.000  \n Securities.Account   CD.Account         Online         CreditCard   \n Min.   :0.0000     Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000     1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000     Median :0.0000   Median :1.0000   Median :0.000  \n Mean   :0.1044     Mean   :0.0604   Mean   :0.5968   Mean   :0.294  \n 3rd Qu.:0.0000     3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000     Max.   :1.0000   Max.   :1.0000   Max.   :1.000"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by Team Dropout Dynamics For INFO 510 FA24 002 Bayesian Inference at the University of Arizona, taught by Dr. Kunal Arekar The team is comprised of the following team members.\n\nPrasanth Gubbala: Pursuing Masters in Information Science/Machine Learning (2023 - 2025) at University of Arizona.\nKendali Beaver: Pursuing Masters in Information Science/Machine Learning (2023 - 2025) at University of Arizona.\nVenkata Satya Murali: Pursuing Masters in Information Science/Machine Learning (2023 - 2025) at University of Arizona."
  }
]