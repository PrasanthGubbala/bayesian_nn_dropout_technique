---
title: "Regularization of Bayesian NN"
subtitle: "Proposal"
author: 
  - name: "Dropout Dynamics"
    affiliations:
      - name: "College of Information Science, University of Arizona"
description: "This project aims to predict the likelihood of a loan applicant being approved or denied using a neural network enhanced with Monte Carlo Dropout regularization. By applying dropout during both training and inference, we aim to improve the model's generalization and estimate prediction uncertainty. The model will be trained on a dataset of 5,000 loan applications, incorporating factors such as age, income, credit score, years of experience, and mortgage value. The project explores how effectively a neural network can predict loan approval based on applicant metadata and the impact of dropout regularization on model performance. Ultimately, this model aims to provide a more reliable and robust decision-making tool for financial institutions."
format:
  html:
    code-tools: true
    code-overflow: wrap
    code-line-numbers: true
    embed-resources: true
editor: visual
code-annotations: hover
execute:
  warning: false
---

```{r}
#| label: load-packages
#| include: false

# Load packages here
pacman::p_load(tidyverse)
```

## Project Goal

The goal of this project is to develop a Bayesian Neural Network, enhanced with the Monte Carlo Dropout regularization technique, to accurately predict the likelihood of a loan applicant being approved or denied. The model will be trained on a dataset containing applicant metadata—such as age, income, credit score, years of experience, and mortgage value—and will predict whether the applicant is likely to be approved for a loan. By using dropout during both training and inference, we aim to improve the model's generalization and estimate prediction uncertainty, ultimately providing a more reliable decision-making tool for financial institutions.

## Questions

1.  How effectively can a neural network predict whether a loan applicant will be approved or denied based on individual financial and demographic metadata?

2.  How does the Monte Carlo Dropout technique impact the performance and generalization of a neural network in predicting loan approval outcomes?

## Dataset

```{r}
#| label: load-dataset
#| message: false

# Load the necessary libraries
library(readr)
library(dplyr)

# Load the data
df <- read_csv("data/bankloan.csv")

# Review all predictors
summary(df)

# Analyze Mortgage predictor
# View mortgage $ data
#range(df$Mortgage) # $0-$635

# Count the number of values greater than 0 in the Mortgage column
#count_above_zero <- sum(df$Mortgage > 0, na.rm = TRUE)

# Get the total number of non-missing values in the column
#total_values <- sum(!is.na(df$Mortgage))

# Calculate the proportion
#proportion_above_zero <- count_above_zero / total_values

# Print the results
#count_above_zero # 1,538 ppl with mortgages
#proportion_above_zero # 30.76% of people have loans

# Remove Mortgage
#df <- select(df, -"Mortgage")

# Normalize using Min-Max Scaling
# df <- df %>%
#   mutate(
#     Age = (Age - min(Age)) / (max(Age) - min(Age)),
#     Experience = (Experience - min(Experience)) / (max(Experience) - min(Experience)),
#     Income = (Income - min(Income)) / (max(Income) - min(Income)),
#     CCAvg = (CCAvg - min(CCAvg)) / (max(CCAvg) - min(CCAvg)),
#     Mortgage = (Mortgage - min(Mortgage, na.rm = TRUE)) / (max(Mortgage, na.rm = TRUE) - min(Mortgage, na.rm = TRUE))
#   )
# 
# summary(df)
```

### Dataset Description

This dataset consists of 5,000 loan applicants with 12 variables that capture demographic, financial, and credit-related factors. It includes information such as age, income, family size, credit score, employment status, and whether the applicant has certain financial accounts (e.g., a credit card or online banking). The target variable indicates whether a personal loan was approved (1) or denied (0).

#### **Dimensions**:

-   **Rows**: 5000 (employees)
-   **Columns**: 12 variables that capture demographic, financial, etc.

#### **Key Variables**:

-   **Numerical**: Age, Income, Years of Experience, Credit Score, Mortgage, etc.
-   **Categorical**: Gender, Job Role, Family Size, Education, Personal Loan Status, etc.

#### **Provenance**:

The dataset is synthetic, designed for research on employee wellbeing, workplace dynamics, and productivity, incorporating common variables seen in employee surveys.

#### **Reason for Choosing**:

It’s relevant for analyzing the impact of work environments (remote, hybrid, onsite) on mental health, work-life balance, and productivity, making it valuable for HR and organizational studies.

## **Priors - Parameters & Hyperparameters:**

For the model, Gaussian distributions are used for priors, aligning with the smoothness and differentiability needed for effective optimization. The key features influencing stress levels, such as age, job role, work location, mental health condition, and hours worked, will determine the priors.

**Parameter Initialization:**

-   **Weights (W_ij):** The weights will be initialized using a Gaussian distribution with: Wij∼N(0,0.5)Wij​∼N(0,0.5)\
    where μ = 0 and σ = 0.5.

-   **Biases (b_j):** The biases will have a smaller variance to remain near zero: bj∼N(0,0.1)bj​∼N(0,0.1)\
    where μ = 0 and σ = 0.1.

**Hyperparameters:**

-   **Model Architecture:** 4 hidden layers with 128, 64, 32, and 16 units, respectively.

-   **Activation Functions:** ReLU for hidden layers, Sigmoid for output layer.

-   **Training Configuration:**

    -   Training data: 80%, Testing data: 20%

    -   Epochs: 30

    -   Batch Size: 5

    -   Loss Function: Binary Cross-Entropy

-   **Optimization:** Adam optimizer, known for its efficiency in handling complex, non-convex optimization problems like neural network training.
